{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from hloc import extract_features, match_features, localize_dma, visualization, pairs_from_exhaustive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for indoor localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Here we declare the paths to the dataset, image pairs, and we choose the feature extractor and the matcher. You need to download the [InLoc dataset](https://www.visuallocalization.net/datasets/) and put it in `datasets/inloc/`, or change the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"drone_2\"\n",
    "\n",
    "dataset = Path(f'../datasets/{data_folder}/')  # change this if your dataset is somewhere else\n",
    "images = Path(f'../datasets/{data_folder}/images')\n",
    "\n",
    "outputs = Path(f'../outputs/{data_folder}/')  # where everything will be saved\n",
    "results = outputs / 'InLoc_hloc_superpoint+superglue_netvlad40.txt'  # the result file\n",
    "\n",
    "loc_pairs = outputs / 'pairs-query.txt'  # top 40 retrieved by NetVLAD\n",
    "features = outputs / 'features.h5'\n",
    "matches = outputs / 'matches.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs for feature extractors:\n",
      "{'d2net-ss': {'model': {'multiscale': False, 'name': 'd2net'},\n",
      "              'output': 'feats-d2net-ss',\n",
      "              'preprocessing': {'grayscale': False, 'resize_max': 1600}},\n",
      " 'dir': {'model': {'name': 'dir'},\n",
      "         'output': 'global-feats-dir',\n",
      "         'preprocessing': {'resize_max': 1024}},\n",
      " 'disk': {'model': {'max_keypoints': 5000, 'name': 'disk'},\n",
      "          'output': 'feats-disk',\n",
      "          'preprocessing': {'grayscale': False, 'resize_max': 1600}},\n",
      " 'netvlad': {'model': {'name': 'netvlad'},\n",
      "             'output': 'global-feats-netvlad',\n",
      "             'preprocessing': {'resize_max': 1024}},\n",
      " 'openibl': {'model': {'name': 'openibl'},\n",
      "             'output': 'global-feats-openibl',\n",
      "             'preprocessing': {'resize_max': 1024}},\n",
      " 'r2d2': {'model': {'max_keypoints': 5000, 'name': 'r2d2'},\n",
      "          'output': 'feats-r2d2-n5000-r1024',\n",
      "          'preprocessing': {'grayscale': False, 'resize_max': 1024}},\n",
      " 'sift': {'model': {'name': 'dog'},\n",
      "          'output': 'feats-sift',\n",
      "          'preprocessing': {'grayscale': True, 'resize_max': 1600}},\n",
      " 'sosnet': {'model': {'descriptor': 'sosnet', 'name': 'dog'},\n",
      "            'output': 'feats-sosnet',\n",
      "            'preprocessing': {'grayscale': True, 'resize_max': 1600}},\n",
      " 'superpoint_aachen': {'model': {'max_keypoints': 4096,\n",
      "                                 'name': 'superpoint',\n",
      "                                 'nms_radius': 3},\n",
      "                       'output': 'feats-superpoint-n4096-r1024',\n",
      "                       'preprocessing': {'grayscale': True,\n",
      "                                         'resize_max': 1024}},\n",
      " 'superpoint_inloc': {'model': {'max_keypoints': 4096,\n",
      "                                'name': 'superpoint',\n",
      "                                'nms_radius': 4},\n",
      "                      'output': 'feats-superpoint-n4096-r1600',\n",
      "                      'preprocessing': {'grayscale': True, 'resize_max': 1600}},\n",
      " 'superpoint_max': {'model': {'max_keypoints': 4096,\n",
      "                              'name': 'superpoint',\n",
      "                              'nms_radius': 3},\n",
      "                    'output': 'feats-superpoint-n4096-rmax1600',\n",
      "                    'preprocessing': {'grayscale': True,\n",
      "                                      'resize_force': True,\n",
      "                                      'resize_max': 1600}}}\n",
      "Configs for feature matchers:\n",
      "{'NN-mutual': {'model': {'do_mutual_check': True, 'name': 'nearest_neighbor'},\n",
      "               'output': 'matches-NN-mutual'},\n",
      " 'NN-ratio': {'model': {'do_mutual_check': True,\n",
      "                        'name': 'nearest_neighbor',\n",
      "                        'ratio_threshold': 0.8},\n",
      "              'output': 'matches-NN-mutual-ratio.8'},\n",
      " 'NN-superpoint': {'model': {'distance_threshold': 0.7,\n",
      "                             'do_mutual_check': True,\n",
      "                             'name': 'nearest_neighbor'},\n",
      "                   'output': 'matches-NN-mutual-dist.7'},\n",
      " 'adalam': {'model': {'name': 'adalam'}, 'output': 'matches-adalam'},\n",
      " 'superglue': {'model': {'name': 'superglue',\n",
      "                         'sinkhorn_iterations': 50,\n",
      "                         'weights': 'outdoor'},\n",
      "               'output': 'matches-superglue'},\n",
      " 'superglue-fast': {'model': {'name': 'superglue',\n",
      "                              'sinkhorn_iterations': 5,\n",
      "                              'weights': 'outdoor'},\n",
      "                    'output': 'matches-superglue-it5'}}\n"
     ]
    }
   ],
   "source": [
    "# list the standard configurations available\n",
    "print(f'Configs for feature extractors:\\n{pformat(extract_features.confs)}')\n",
    "print(f'Configs for feature matchers:\\n{pformat(match_features.confs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one of the configurations for extraction and matching\n",
    "# you can also simply write your own here!\n",
    "feature_conf = extract_features.confs['superpoint_inloc']\n",
    "matcher_conf = match_features.confs['superglue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract local features for database and query images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/12/17 14:20:53 hloc INFO] Extracting local features with configuration:\n",
      "{'model': {'max_keypoints': 4096, 'name': 'superpoint', 'nms_radius': 4},\n",
      " 'output': 'feats-superpoint-n4096-r1600',\n",
      " 'preprocessing': {'grayscale': True, 'resize_max': 1600}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 12/12 [00:02<00:00,  5.70it/s]\n",
      "[2022/12/17 14:20:57 hloc INFO] Finished exporting features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/drone_2/features.h5\n"
     ]
    }
   ],
   "source": [
    "#feature_path = extract_features.main(feature_conf, dataset, pairs)\n",
    "#extract_features.main(feature_conf, images, image_list=references, feature_path=features)\n",
    "#pairs_from_exhaustive.main(sfm_pairs, image_list=references)\n",
    "#match_features.main(matcher_conf, sfm_pairs, features=features, matches=matches);\n",
    "\n",
    "references = [p.relative_to(images).as_posix() for p in (images / 'mapping/altitude_4/').iterdir()]\n",
    "\n",
    "features = extract_features.main(feature_conf, images, \n",
    "                      image_list=references, \n",
    "                      feature_path=features)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the query images\n",
    "Here we assume that the localization pairs are already computed using image retrieval (NetVLAD). To generate new pairs from your own global descriptors, have a look at `hloc/pairs_from_retrieval.py`. These pairs are also used for the localization - see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/12/17 14:21:02 hloc INFO] Extracting local features with configuration:\n",
      "{'model': {'max_keypoints': 4096, 'name': 'superpoint', 'nms_radius': 4},\n",
      " 'output': 'feats-superpoint-n4096-r1600',\n",
      " 'preprocessing': {'grayscale': True, 'resize_max': 1600}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.11it/s]\n",
      "[2022/12/17 14:21:02 hloc INFO] Finished exporting features.\n",
      "[2022/12/17 14:21:02 hloc INFO] Found 72 pairs.\n",
      "[2022/12/17 14:21:02 hloc INFO] Matching local features with configuration:\n",
      "{'model': {'name': 'superglue',\n",
      "           'sinkhorn_iterations': 50,\n",
      "           'weights': 'outdoor'},\n",
      " 'output': 'matches-superglue'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/drone_2/pairs-query.txt ../outputs/drone_2/features.h5 ../outputs/drone_2/matches.h5\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.29it/s]\n",
      "[2022/12/17 14:21:12 hloc INFO] Finished exporting matches.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('../outputs/drone_2/matches.h5')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query_image_path = 'query/DJI_0057.JPG'\n",
    "\n",
    "queries = [p.relative_to(images).as_posix() for p in (images / 'query/').iterdir()]\n",
    "\n",
    "features = extract_features.main(feature_conf, images, \n",
    "                      image_list=queries, \n",
    "                      feature_path=features)\n",
    "\n",
    "pairs_from_exhaustive.main(loc_pairs, image_list=queries, ref_list=references)\n",
    "\n",
    "#match_path = match_features.main(matcher_conf, loc_pairs, feature_conf['output'], outputs)\n",
    "print(loc_pairs, features, matches)\n",
    "match_features.main(matcher_conf, loc_pairs, features=features, matches=matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localize!\n",
    "Perform hierarchical localization using the precomputed retrieval and matches. Different from when localizing with Aachen, here we do not need a 3D SfM model here: the dataset already has 3D lidar scans. The file `InLoc_hloc_superpoint+superglue_netvlad40.txt` will contain the estimated query poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/12/17 14:27:28 hloc INFO] Starting localization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query/altitude=4.0_01361.jpg', 'query/altitude=4.0_01362.jpg', 'query/altitude=4.0_01363.jpg', 'query/altitude=4.0_01364.jpg', 'query/altitude=4.0_01365.jpg', 'query/altitude=4.0_01366.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                             | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/drone_2/images\n",
      "0 0\n",
      "../datasets/drone_2/images/poses/0_camera_info.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 9 into shape (4,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlocalize_dma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_matches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# skip database images with too few matches\u001b[39;00m\n",
      "File \u001b[0;32m/app/notebooks/../hloc/localize_dma.py:465\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dataset_dir, retrieval, features, matches, results, skip_matches)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m tqdm(queries):\n\u001b[1;32m    464\u001b[0m     db \u001b[38;5;241m=\u001b[39m retrieval_dict[q]\n\u001b[0;32m--> 465\u001b[0m     ret, mkpq, mkpr, mkp3d, indices, num_matches \u001b[38;5;241m=\u001b[39m \u001b[43mpose_from_cluster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_matches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/app/notebooks/../hloc/localize_dma.py:211\u001b[0m, in \u001b[0;36mpose_from_cluster\u001b[0;34m(dataset_dir, q, retrieved, feature_file, match_file, skip)\u001b[0m\n\u001b[1;32m    209\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworldPose\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m     t \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(tp) \u001b[38;5;28;01mfor\u001b[39;00m tp \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m--> 211\u001b[0m     worldPose \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mprint\u001b[39m(worldPose)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 9 into shape (4,4)"
     ]
    }
   ],
   "source": [
    "localize_dma.main(\n",
    "    dataset, loc_pairs, features, matches, results,\n",
    "    skip_matches=20)  # skip database images with too few matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "We parse the localization logs and for each query image plot matches and inliers with a few database images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "visualization.visualize_loc(results, images, n=len(queries), top_k_db=1, seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize camera poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_query_camera_poses(result_path):\n",
    "    query_poses = {}\n",
    "    with open(result_path) as file:\n",
    "        q_name = \"\"\n",
    "        for line in file:\n",
    "            line_strip = line.rstrip()\n",
    "            if line_strip.islower() or line_strip.isupper():\n",
    "                splitted_line = line_strip.split(\" \")\n",
    "                name = splitted_line[0]\n",
    "                q_name = name\n",
    "                query_poses[q_name] = {}\n",
    "            else:\n",
    "                Rt = np.zeros((4,4))\n",
    "                rt = line_strip.split(\" \")\n",
    "                rt = [float(r) for r in rt]\n",
    "                rt = np.array(rt).reshape((4,4))\n",
    "                Rt = rt\n",
    "                #print(Rt)\n",
    "                query_poses[q_name][\"Rt\"] = Rt\n",
    "                query_poses[q_name][\"R\"] = Rt[:3, :3]\n",
    "                query_poses[q_name][\"t\"] = Rt[:3, -1]\n",
    "    return query_poses\n",
    "\n",
    "gallery_camera_poses = localize_dma.get_all_camera_poses(images)\n",
    "query_est_poses = get_query_camera_poses(results)\n",
    "\n",
    "gal_translations = np.array([gallery_camera_poses[key][\"translation\"] for key in gallery_camera_poses.keys()])\n",
    "q_translations = np.array([query_est_poses[key][\"t\"] for key in query_est_poses.keys()])\n",
    "#print(gallery_camera_poses)\n",
    "#print(q_translations)\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "gx = gal_translations[:, 0]\n",
    "gy = gal_translations[:, 1]\n",
    "gz = gal_translations[:, 2]\n",
    "\n",
    "qx = q_translations[:, 0]\n",
    "qy = q_translations[:, 1]\n",
    "qz = q_translations[:, 2]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.scatter(gx, gz, gy, color=\"b\")\n",
    "ax.scatter(qx, qz, qy, color=\"r\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib --list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from hloc.utils.camera_pose_visualizer import CameraPoseVisualizer\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "# argument : the minimum/maximum value of x, y, z\n",
    "keys = np.array(list(gallery_camera_poses.keys()))\n",
    "print(np.sort(keys))\n",
    "gal_ext = np.array([gallery_camera_poses[key][\"worldPose\"] for key in np.sort(keys)])\n",
    "\n",
    "q_ext = np.array([query_est_poses[key][\"Rt\"] for key in query_est_poses.keys()])\n",
    "\n",
    "visualizer = CameraPoseVisualizer([-4, 3], [-6, 2], [-1, 1])\n",
    "\n",
    "# argument : extrinsic matrix, color, scaled focal length(z-axis length of frame body of camera\n",
    "for ext in gal_ext:\n",
    "    print(ext)\n",
    "#     flip_YZ = np.eye(4)\n",
    "#     flip_YZ[1,1] = -1\n",
    "#     flip_YZ[2,2] = -1\n",
    "#     ext = ext @ flip_YZ\n",
    "    visualizer.extrinsic2pyramid(ext, 'c', 0.5, 0.5)\n",
    "    \n",
    "print(\"\\nQUERY POSES:\")\n",
    "for ext in q_ext:\n",
    "    print(ext)\n",
    "#     flip_YZ = np.eye(4)\n",
    "#     flip_YZ[1,1] = -1\n",
    "#     flip_YZ[2,2] = -1\n",
    "#     ext = ext @ flip_YZ\n",
    "    visualizer.extrinsic2pyramid(ext, 'r', 0.5, 0.5)\n",
    "#print(np.eye(4))\n",
    "#visualizer.extrinsic2pyramid(gal_ext[0], 'c', 10)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "\n",
    "import cv2\n",
    "\n",
    "img_idx = '00'\n",
    "img_path = f'./images/mapping/rgb_1920x1440_000{img_idx}.jpeg'\n",
    "\n",
    "query_inliers = np.load(f\"../third_party/PatchNetVLAD/patchnetvlad/inlier_indices/drone_2/query/inlier_indices_rgb_1920x1440_000{img_idx}.npy\", allow_pickle=True)\n",
    "print(f'../datasets/drone_2/images/mapping/rgb_1920x1440_000{img_idx}.jpeg')\n",
    "qheight, qwidth = cv2.imread(f'../datasets/drone_2/images/mapping/rgb_1920x1440_000{img_idx}.jpeg').shape[:2]\n",
    "result_path = \"../third_party/PatchNetVLAD/patchnetvlad/results/drone_2/PatchNetVLAD_predictions.txt\"\n",
    "\n",
    "db_images = []\n",
    "with open(result_path) as file:\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        if line.__contains__(img_path):\n",
    "            t = line.rstrip()\n",
    "            t = t.split(\", \")[1]\n",
    "            t = t.split(\"./\")[-1]\n",
    "            best_img = t.split(\"/\")[-1].replace(\".jpg\", \"\")\n",
    "            db_images.append(best_img)\n",
    "print(db_images)\n",
    "for index, img_name in enumerate(db_images[:3]):\n",
    "    \n",
    "    db_inliers = np.load(f\"../third_party/PatchNetVLAD/patchnetvlad/inlier_indices/drone_2/index/inlier_indices_rgb_1920x1440_000{img_idx}.npy\", allow_pickle=True)\n",
    "    print(f'../datasets/drone_2/images/query/{img_name}.jpg')\n",
    "    dbheight, dbwidth = cv2.imread(f'../datasets/drone_2/images/query/{img_name}.jpg').shape[:2]\n",
    "    kp_height, kp_width = 480, 640 # height, width\n",
    "    \n",
    "    print(db_inliers.shape)\n",
    "    \n",
    "    p2_size=2\n",
    "    p5_size=5\n",
    "    p8_size=8\n",
    "    \n",
    "    patchdb10 = np.array([int((p2_size / kp_height) * dbheight), int((p2_size / kp_width) * dbwidth)])\n",
    "    patchdb15 = np.array([int((p5_size / kp_height) * dbheight), int((p5_size / kp_width) * dbwidth)])\n",
    "    patchdb18 = np.array([int((p8_size / kp_height) * dbheight), int((p8_size / kp_width) * dbwidth)])\n",
    "\n",
    "    resized_db_path_sizes = np.array([patchdb10, patchdb15, patchdb18])\n",
    "    \n",
    "    patchq10 = np.array([int((p2_size / kp_height) * qheight), int((p2_size / kp_width) * qwidth)])\n",
    "    patchq15 = np.array([int((p5_size / kp_height) * qheight), int((p5_size / kp_width) * qwidth)])\n",
    "    patchq18 = np.array([int((p8_size / kp_height) * qheight), int((p8_size / kp_width) * qwidth)])\n",
    "\n",
    "    resized_q_path_sizes = np.array([patchq10, patchq15, patchq18])\n",
    "    print(resized_path_sizes)\n",
    "    print(img_name)\n",
    "\n",
    "    qp2, qp5, qp8 = query_inliers[index][0], query_inliers[index][1], query_inliers[index][2]\n",
    "    dbp2, dbp5, dbp8 = db_inliers[index][0], db_inliers[index][1], db_inliers[index][2]\n",
    "    \n",
    "    print(qp2.shape, qp5.shape, qp8.shape)\n",
    "    print(dbp2.shape, dbp5.shape, dbp8.shape)\n",
    "    \n",
    "    if qp2.shape[0] == 0 or qp5.shape[0] == 0 or qp8.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    # convert points from one resolution to another\n",
    "    resized_qp2 = np.array([np.array([int((kp[1] / kp_height) * qheight), int((kp[0] / kp_width) * qwidth)]) for kp in qp2])\n",
    "    resized_qp5 = np.array([np.array([int((kp[1] / kp_height) * qheight), int((kp[0] / kp_width) * qwidth)]) for kp in qp5])\n",
    "    resized_qp8 = np.array([np.array([int((kp[1] / kp_height) * qheight), int((kp[0] / kp_width) * qwidth)]) for kp in qp8])\n",
    "\n",
    "    resized_dbp2 = np.array([np.array([int((kp[1] / kp_height) * dbheight), int((kp[0] / kp_width) * dbwidth)]) for kp in dbp2])\n",
    "    resized_dbp5 = np.array([np.array([int((kp[1] / kp_height) * dbheight), int((kp[0] / kp_width) * dbwidth)]) for kp in dbp5])\n",
    "    resized_dbp8 = np.array([np.array([int((kp[1] / kp_height) * dbheight), int((kp[0] / kp_width) * dbwidth)]) for kp in dbp8])\n",
    "    \n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(8, 6))\n",
    "    qimg = mpimg.imread(f'../datasets/drone_2/images/mapping/rgb_1920x1440_000{img_idx}.jpeg')\n",
    "    axs[0].set_title(f'query: rgb_1920x1440_000{img_idx}', fontsize=8)\n",
    "    axs[0].imshow(qimg)\n",
    "    \n",
    "    xyp2As, xyp2Bs, xyp5As, xyp5Bs, xyp8As, xyp8Bs = [], [], [], [], [], []\n",
    "    \n",
    "    for kp in resized_qp2:\n",
    "        xyp2As.append((kp[1], kp[0]))\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((kp[1]-(resized_q_path_sizes[0][1]/2), kp[0]-(resized_q_path_sizes[0][0]/2)), resized_q_path_sizes[0][1], resized_q_path_sizes[0][0], linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        axs[0].add_patch(rect)\n",
    "        \n",
    "    for kp in resized_qp5:\n",
    "        xyp5As.append((kp[1], kp[0]))\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((kp[1]-(resized_q_path_sizes[1][1]/2), kp[0]-(resized_q_path_sizes[1][0]/2)), resized_q_path_sizes[1][1], resized_q_path_sizes[1][0], linewidth=1, edgecolor='c', facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        axs[0].add_patch(rect)\n",
    "        \n",
    "    for kp in resized_qp8:\n",
    "        xyp8As.append((kp[1], kp[0]))\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((kp[1]-(resized_q_path_sizes[2][1]/2), kp[0]-(resized_q_path_sizes[2][0]/2)), resized_q_path_sizes[2][1], resized_q_path_sizes[2][0], linewidth=1, edgecolor='g', facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        axs[0].add_patch(rect)\n",
    "        \n",
    "    \n",
    "    axs[0].scatter(resized_qp2[:,1], resized_qp2[:,0], marker=\"o\", c='r', s=2)\n",
    "    axs[0].scatter(resized_qp5[:,1], resized_qp5[:,0], marker=\"o\", c='c', s=2)\n",
    "    axs[0].scatter(resized_qp8[:,1], resized_qp8[:,0], marker=\"o\", c='g', s=2)\n",
    "\n",
    "\n",
    "    dbimg = mpimg.imread(f'../datasets/drone_2/images/query/{img_name}.jpg')\n",
    "    axs[1].set_title(f'db: {img_name}', fontsize=8)\n",
    "    axs[1].imshow(dbimg)\n",
    "    \n",
    "    for kp in resized_dbp2:\n",
    "        xyp2Bs.append((kp[1], kp[0]))\n",
    "        rect = patches.Rectangle((kp[1]-(resized_db_path_sizes[0][1]/2), kp[0]-(resized_db_path_sizes[0][0]/2)), resized_db_path_sizes[0][1], resized_db_path_sizes[0][0], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        axs[1].add_patch(rect)\n",
    "        \n",
    "    for kp in resized_dbp5:\n",
    "        xyp5Bs.append((kp[1], kp[0]))\n",
    "        rect = patches.Rectangle((kp[1]-(resized_db_path_sizes[1][1]/2), kp[0]-(resized_db_path_sizes[1][0]/2)), resized_db_path_sizes[1][1], resized_db_path_sizes[1][0], linewidth=1, edgecolor='c', facecolor='none')\n",
    "        axs[1].add_patch(rect)\n",
    "        \n",
    "    for kp in resized_dbp8:\n",
    "        xyp8Bs.append((kp[1], kp[0]))\n",
    "        rect = patches.Rectangle((kp[1]-(resized_db_path_sizes[2][1]/2), kp[0]-(resized_db_path_sizes[2][0]/2)), resized_db_path_sizes[2][1], resized_db_path_sizes[2][0], linewidth=1, edgecolor='g', facecolor='none')\n",
    "        axs[1].add_patch(rect)\n",
    "        \n",
    "    axs[1].scatter(resized_dbp2[:,1], resized_dbp2[:,0], marker=\",\", c='r', s=2)\n",
    "    axs[1].scatter(resized_dbp5[:,1], resized_dbp5[:,0], marker=\",\", c='c', s=2)\n",
    "    axs[1].scatter(resized_dbp8[:,1], resized_dbp8[:,0], marker=\",\", c='g', s=2)\n",
    "    \n",
    "    axs[0].set_xticks([])\n",
    "    axs[0].set_yticks([])\n",
    "    axs[1].set_xticks([])\n",
    "    axs[1].set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    for xyIdx, xy in enumerate(xyp2As):\n",
    "        con = ConnectionPatch(linewidth=0.5, xyA=xy, xyB=xyp2Bs[xyIdx], coordsA=\"data\", coordsB=\"data\",\n",
    "                              axesA=axs[0], axesB=axs[1], color=\"red\")\n",
    "        axs[1].add_artist(con)\n",
    "        \n",
    "    for xyIdx, xy in enumerate(xyp5As):\n",
    "        con = ConnectionPatch(linewidth=0.5, xyA=xy, xyB=xyp5Bs[xyIdx], coordsA=\"data\", coordsB=\"data\",\n",
    "                              axesA=axs[0], axesB=axs[1], color=\"cyan\")\n",
    "        axs[1].add_artist(con)\n",
    "        \n",
    "    for xyIdx, xy in enumerate(xyp8As):\n",
    "        con = ConnectionPatch(linewidth=0.5, xyA=xy, xyB=xyp8Bs[xyIdx], coordsA=\"data\", coordsB=\"data\",\n",
    "                              axesA=axs[0], axesB=axs[1], color=\"green\")\n",
    "        axs[1].add_artist(con)\n",
    "\n",
    "#print(resized_p2, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib  \n",
    "# pnvlad_feature_extract = importlib.import_module(\"third_party.Patch-NetVLAD.feature_extract\")\n",
    "\n",
    "# pnvlad_feature_extract()\n",
    "\n",
    "#sys.path.append(str('../third_party/PatchNetVLAD'))\n",
    "#%pip install faiss-gpu\n",
    "\n",
    "\n",
    "#from third_party.PatchNetVLAD import feature_extract as pnv_feature_extract\n",
    "\n",
    "\n",
    "#pnv_feature_extract.main()\n",
    "\n",
    "#python feature_extract.py \\\n",
    "#  --config_path patchnetvlad/configs/performance.ini \\\n",
    "#  --dataset_file_path=pitts30k_imageNames_index.txt \\\n",
    "#  --dataset_root_dir=/path/to/your/pitts/dataset \\\n",
    "#  --output_features_dir patchnetvlad/output_features/pitts30k_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
